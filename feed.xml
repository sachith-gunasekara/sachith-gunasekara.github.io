<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://sachith-gunasekara.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sachith-gunasekara.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-18T20:11:01+00:00</updated><id>https://sachith-gunasekara.github.io/feed.xml</id><title type="html">blank</title><subtitle>Welcome to the personal website of Sachith Gunasekara. </subtitle><entry><title type="html">nanoJAXGPT — A pedagogical introduction to JAX/Equinox</title><link href="https://sachith-gunasekara.github.io/blog/2024/nanoJAXGPT/" rel="alternate" type="text/html" title="nanoJAXGPT — A pedagogical introduction to JAX/Equinox"/><published>2024-10-23T09:00:00+00:00</published><updated>2024-10-23T09:00:00+00:00</updated><id>https://sachith-gunasekara.github.io/blog/2024/nanoJAXGPT</id><content type="html" xml:base="https://sachith-gunasekara.github.io/blog/2024/nanoJAXGPT/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Since its introduction, <a href="https://jax.readthedocs.io/en/latest/index.html"><em>JAX</em></a> has seen a significant rise in popularity within the Machine Learning (ML) community. A simple web search would reveal the vast community support, a variety of derivative projects, and a multitude of <em>Python</em> libraries built around <em>JAX</em>. This leads to the inevitable question: What is <em>JAX</em>, and why should I care?</p> <blockquote> Well, according to the official documentation, <i>JAX</i> is a <i>Python</i> library for accelerator-oriented array computation and…</blockquote> <p>Wait a minute, let’s pump the brakes here! If you were really after the introduction to <em>JAX</em> as outlined in the official docs, you’d be there, not here on this blog post. That being said, while there are plenty of resources to help you kick off your machine learning projects with <em>JAX</em>, this article isn’t just about singing praises for <em>JAX</em> as an ML framework nor introducing ML to beginners using it. We’re going to roll up our sleeves and get hands-on, taking a well-known repository (Andrej Karpathy’s <em>nanoGPT</em>) and rewriting it from top to bottom using <em>JAX</em> and <em>Equinox</em>.</p> <h3 id="ummequinox">Umm…<em>Equinox</em>?</h3> <p>Yes, if you haven’t heard of this already, <em>Equinox</em> is a library built around <em>JAX</em> with the aim of making the construction of Neural Networks (NN) as smooth as possible. What sets it apart is its familiar <em>PyTorch</em>-like syntax, making it a comfortable transition for those coming from a <em>PyTorch</em> background. But don’t be fooled by its simplicity. Underneath the hood, Equinox is diligently registering your model as a <a href="https://jax.readthedocs.io/en/latest/pytrees.html"><em>JAX PyTree</em></a>, a powerful data structure in <em>JAX</em> that allows for complex transformations and computations.</p> <p>To put it all in context, we’ll illustrate this process through a practical example. Here’s a snippet of code that demonstrates how you can define a Linear layer using <em>Equinox</em>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Code extracted from https://docs.kidger.site/equinox/all-of-equinox/
</span>
<span class="kn">import</span> <span class="n">equinox</span> <span class="k">as</span> <span class="n">eqx</span>
<span class="kn">import</span> <span class="n">jax</span>

<span class="k">class</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">eqx</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">weight</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="n">wkey</span><span class="p">,</span> <span class="n">bkey</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">wkey</span><span class="p">,</span> <span class="p">(</span><span class="n">out_size</span><span class="p">,</span> <span class="n">in_size</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">bkey</span><span class="p">,</span> <span class="p">(</span><span class="n">out_size</span><span class="p">,))</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">@</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span>
</code></pre></div></div> <p>Now, <em>Equinox</em> offers a variety of prebuilt neural network layers, including the <em>Linear</em> layer that we just defined above, that can be utilized to construct complex architectures. A distinctive advantage of <em>Equinox</em> as a library for training deep learning models with <em>JAX</em> is its ability to incorporate arbitrary <em>Python</em> objects, more specifically activation functions, into the <em>PyTree</em> definition. It also provides additional functionality to facilitate the use of <em>JAX</em>’s <code class="language-plaintext highlighter-rouge">jax.jit</code> and <code class="language-plaintext highlighter-rouge">jax.grad</code> decorators, given that they require all inputs to be <em>PyTrees</em> of arrays, by implementing filtered transformations as <code class="language-plaintext highlighter-rouge">equinox.filter_jit</code> and <code class="language-plaintext highlighter-rouge">equinox.filter_grad</code> decorators respectively. You can find more information on filtering in <em>Equinox</em> <a href="https://docs.kidger.site/equinox/all-of-equinox/#2-filtering">here</a>.</p> <h2 id="prerequisites">Prerequisites</h2> <p>The following sections of this blog assume that you, the reader possesses a foundational understanding of <em>JAX</em>. Below, we compile a comprehensive, yet not exhaustive, list of resources to help you get started.</p> <ul> <li>JAX introduction tutorial notebooks <ul> <li><a style=" display: inline-block; padding: 6px 12px; color: #3a8bdb; border: 1px solid #384148; border-radius: 5px; text-decoration: none; " href="https://jax.readthedocs.io/en/latest/tutorials/"> Tutorials — JAX documentation </a></li> </ul> </li> <li>Thinking in JAX <ul> <li><a style=" display: inline-block; padding: 6px 12px; color: #3a8bdb; border: 1px solid #384148; border-radius: 5px; text-decoration: none; " href="https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html"> How to think in JAX — JAX documentation </a></li> </ul> </li> <li>JAX Automatic vectorization <ul> <li><a style=" display: inline-block; padding: 6px 12px; color: #3a8bdb; border: 1px solid #384148; border-radius: 5px; text-decoration: none; " href="https://jax.readthedocs.io/en/latest/automatic-vectorization.html"> Automatic vectorization — JAX documentation </a></li> <li><a style=" display: inline-block; padding: 6px 12px; color: #3a8bdb; border: 1px solid #384148; border-radius: 5px; text-decoration: none; " href="https://dinocausevic.com/2023/06/13/jax-vmap/"> JAX VMAP Simplified: An Easy Introduction for Beginners </a></li> <li><a style=" display: inline-block; padding: 6px 12px; color: #3a8bdb; border: 1px solid #384148; border-radius: 5px; text-decoration: none; " href="https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html"> jax.vmap — JAX documentation </a></li> </ul> </li> <li>Custom parameter initialization in Equinox <ul> <li><a style=" display: inline-block; padding: 6px 12px; color: #3a8bdb; border: 1px solid #384148; border-radius: 5px; text-decoration: none; " href="https://docs.kidger.site/equinox/tricks/#custom-parameter-initialisation"> Tricks (ensembles, surgery, custom initializations, …) - Equinox </a></li> </ul> </li> </ul> <h2 id="notes-for-clarity">Notes for Clarity</h2> <ul> <li>In PyTorch, the conventional practice is to define a <code class="language-plaintext highlighter-rouge">forward</code> method in modules, which is designed to perform actions during the forward pass of the training phase. This approach could be employed in equinox modules as well. However, it is also typical to define the computations for the forward pass within the <code class="language-plaintext highlighter-rouge">__call__</code> definition of the class. This provides an easy way to define a forward pass for a model, but it’s important to note that any method can be used, and no methods are special-cased. Therefore, in the context of the upcoming sections, when we refer to the forward pass, it is suggested that the reader’s attention be directed towards the <code class="language-plaintext highlighter-rouge">__call__</code> definition of the respective module, or any other method that the developer chooses to use for this purpose.</li> </ul> <h2 id="nanogpt">nanoGPT</h2> <p><a href="https://github.com/karpathy/nanoGPT">nanoGPT</a> is a simple and fast repository for training or finetuning medium sized GPTs (<a href="https://en.wikipedia.org/wiki/Generative_pre-trained_transformer">Generative Pretrained Transformer</a>). This is the deep learning repository that we will be rewriting with JAX/Equinox. The contents of this repository is shown in Figure 1 of which we emphasize on <code class="language-plaintext highlighter-rouge">model.py</code> and <code class="language-plaintext highlighter-rouge">train.py</code>.</p> <p align="center"> <img src="https://cdn-uploads.huggingface.co/production/uploads/647eff9aaa8c04bbf9365219/vEA6dJ6XKpyMWleo5N4KW.png" alt="Description of the image" style="height: 500px;"/> <br/> <i>Fig1: Project structure of nanoGPT</i> </p> <hr/> <h3 id="modelpy"><code class="language-plaintext highlighter-rouge">model.py</code></h3> <p>The model outlined in this file draws inspiration from the <a href="https://openai.com/research/gpt-2-1-5b-release">GPT-2</a> architecture, incorporating various modules to emulate a comparable structure. It is designed to be accessible and comprehensible, even for those new to the field. Let us first outline the most significant modules found in this model definition below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CausalSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="bp">...</span>

<span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="bp">...</span>

<span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="bp">...</span>

<span class="k">class</span> <span class="nc">GPT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="bp">...</span>
</code></pre></div></div> <h3 id="trainpy"><code class="language-plaintext highlighter-rouge">train.py</code></h3> <p>With the defined model architecture in the <code class="language-plaintext highlighter-rouge">model.py</code> file, within this file resides a training script to train the model using <em>PyTorch</em>. You may observe the contents of this file in the orginal repository linked above. Since the training paradigm in <em>JAX</em> is quite different to that in <em>PyTorch</em>, we do not extract and outline the structure of this file here.</p> <h2 id="rewriting-modelpy">Rewriting <code class="language-plaintext highlighter-rouge">model.py</code></h2> <h3 id="introducing-swiglu-to-nanogpt">Introducing <em>SwiGLU</em> to <em>nanoGPT</em></h3> <p>In our effort to rewrite <em>nanoGPT</em>, we sought to introduce a unique element to the final output. To this end, we implemented the <a href="https://paperswithcode.com/method/swiglu"><em>SwiGLU</em></a> activation function in place of the standard <a href="https://paperswithcode.com/method/gelu#:~:text=The%20Gaussian%20Error%20Linear%20Unit,standard%20Gaussian%20cumulative%20distribution%20function."><em>GELU</em></a> activation within the MLP module. <em>SwiGLU</em>, a variant of the <a href="https://paperswithcode.com/method/glu"><em>GLU</em></a> activation function, is notable for its ability to dynamically adjust non-linearity based on the specific training task. For those interested in delving deeper into <em>SwiGLU</em>, additional information can be found <a href="https://deci.ai/blog/evolution-of-modern-transformer-swiglu-rope-gqa-attention-is-all-you-need/#:~:text=SwiGLU%3A%20An%20Enhanced%20Activation%20Function%20for%20Better%20Performance">here</a>.</p> <p>The mathematical representation of the <em>SwiGLU</em> activation function is as follows: \(SwiGLU(x, W, V, b, c, \beta) = Swish_{\beta}(xW + b) \otimes (xV + c)\)</p> <p>Here \(W, V, b, c\) are all trainable parameters in the neural network, and we can implement this as shown in the codeblock below. Let us try to breakdown this code step-by-step:</p> <ul> <li>We first create a subclass of the <code class="language-plaintext highlighter-rouge">eqx.Module</code> class as this activation function has trainable parameters, and hence we need to register this in our <em>PyTree</em> definition.</li> <li>We define the <code class="language-plaintext highlighter-rouge">__init__</code> method with the three parameters <code class="language-plaintext highlighter-rouge">dim_in</code>, <code class="language-plaintext highlighter-rouge">dim_out</code>, and <code class="language-plaintext highlighter-rouge">key</code>. The first two must be defined during the time of initializing of this module and we will infer the appropriate values based on the input and output number of parameters respectively.</li> <li>The <code class="language-plaintext highlighter-rouge">__call__</code> method implements the definition of the SwiGLU activation function. We apply the <a href="https://paperswithcode.com/method/swish"><em>Swish</em></a> activation function on one transformation of the input and carry out a component-wise multiplication with another transformation of the input.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">equinox</span> <span class="k">as</span> <span class="n">eqx</span>
<span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="k">class</span> <span class="nc">SwiGLU</span><span class="p">(</span><span class="n">eqx</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Implementation of the SwiGLU activation function in the paper by Noam Shazeer at Google

    References:
        GLU Variants Improve Transformer paper  : https://arxiv.org/abs/2002.05202
        Aziz et al. Paper Summaries             : https://azizbelaweid.substack.com/p/what-is-swiglu-how-to-implement-it
    </span><span class="sh">"""</span>

    <span class="n">W</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span>
    <span class="n">V</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span>
    <span class="n">c</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="n">k1</span><span class="p">,</span> <span class="n">k2</span><span class="p">,</span> <span class="n">k3</span><span class="p">,</span> <span class="n">k4</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">k1</span><span class="p">,</span> <span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">k2</span><span class="p">,</span> <span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">k3</span><span class="p">,</span> <span class="p">(</span><span class="n">dim_out</span><span class="p">,))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">k4</span><span class="p">,</span> <span class="p">(</span><span class="n">dim_out</span><span class="p">,))</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">swish</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">b</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">V</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">c</span><span class="p">)</span>
</code></pre></div></div> <div style=" background-color: #1c2b41; border-radius: 5px; padding: 10px; margin: 20px 0; display: flex; align-items: flex-start; "> <img src="https://cdn-uploads.huggingface.co/production/uploads/647eff9aaa8c04bbf9365219/S7GC6ed_inRwpFT2-S4sC.png" alt="Icon" style=" background-color: transparent; margin: 0 10px 0 0; height: 25px; border: none; align-self: flex-start; "/> <p style="margin: 0; line-height: 1.5; color: #b3bcc9;"> In most of the upcoming modules, you may notice that there is a <code>config</code> parameter. We pass in a <code>dataclass</code> object initialized from the following <code>GPTConfig</code> definition as an argument to this parameter. It contains a predefined configuration of the architecture of the model. </p> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">GPTConfig</span><span class="p">:</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50304</span>  <span class="c1"># GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
</span>    <span class="n">n_layer</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span>
    <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span>
    <span class="n">n_embd</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">768</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span>  <span class="c1"># True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
</span></code></pre></div></div> <h3 id="mlp-module">MLP Module</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">c_fc</span>    <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gelu</span>    <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">c_proj</span>  <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">c_fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">c_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p>Given our gathered experience in constructing a module from scratch with <code class="language-plaintext highlighter-rouge">equinox</code>, the process of converting the aforementioned <em>MLP</em> layer should be relatively straightforward. We outline the steps for this conversion as follows:</p> <ol> <li>Firstly, change this class into an <code class="language-plaintext highlighter-rouge">equinox</code> module from <code class="language-plaintext highlighter-rouge">torch.nn</code>. <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">eqx</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
</code></pre></div> </div> </li> <li>Next, let’s rewrite the <code class="language-plaintext highlighter-rouge">__init__</code> method to initialize the <em>MLP</em> layer in <em>JAX</em>. We’ve replaced the <em>PyTorch</em> <code class="language-plaintext highlighter-rouge">nn.Linear</code> and <code class="language-plaintext highlighter-rouge">nn.Dropout</code> layers with their <em>Equinox</em> equivalents, keeping the arguments consistent to preserve the original behavior. We initialize the <code class="language-plaintext highlighter-rouge">SwiGLU</code> module in our <em>Equinox</em> version, carefully selecting the <code class="language-plaintext highlighter-rouge">dim_in</code> and <code class="language-plaintext highlighter-rouge">dim_out</code> arguments to match the output dimension of the preceding <em>Linear</em> layer and the input dimension of the subsequent <em>Linear</em> layer, both being <code class="language-plaintext highlighter-rouge">4 * config.n_embd</code>. <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">eqx</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
     <span class="n">c_fc</span>    <span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span>
     <span class="n">swiglu</span>  <span class="p">:</span> <span class="n">SwiGLU</span>
     <span class="n">c_proj</span>  <span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span>
     <span class="n">dropout</span> <span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span>
    
     <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
         <span class="n">lkey1</span><span class="p">,</span> <span class="n">lkey2</span><span class="p">,</span> <span class="n">skey</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    
         <span class="n">self</span><span class="p">.</span><span class="n">c_fc</span>     <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">lkey1</span><span class="p">)</span>
         <span class="n">self</span><span class="p">.</span><span class="n">swiglu</span>   <span class="o">=</span> <span class="nc">SwiGLU</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">skey</span><span class="p">)</span>
         <span class="n">self</span><span class="p">.</span><span class="n">c_proj</span>   <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">lkey2</span><span class="p">)</span>
         <span class="n">self</span><span class="p">.</span><span class="n">dropout</span>  <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <p>Lastly, we’ve replaced the activation function <code class="language-plaintext highlighter-rouge">self.gelu(x)</code> with <code class="language-plaintext highlighter-rouge">self.swiglu(x)</code> in the forward pass. As you may have observed, we have employed a transformation function, <code class="language-plaintext highlighter-rouge">jax.vmap</code>, during certain steps of the forward pass. This will be further elaborated when we dissect the entire architecture in a layer-by-layer manner, explaining the dimensions of the input that each module receives and the necessity of a <code class="language-plaintext highlighter-rouge">vmap</code> in such a context.</p> <p>However, for the time being, let’s continue rewriting the remaining modules in our model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">eqx</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
     <span class="n">c_fc</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span>
     <span class="n">swiglu</span><span class="p">:</span> <span class="n">SwiGLU</span>
     <span class="n">c_proj</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span>
     <span class="n">dropout</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span>
    
     <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
         <span class="n">lkey1</span><span class="p">,</span> <span class="n">lkey2</span><span class="p">,</span> <span class="n">skey</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    
         <span class="n">self</span><span class="p">.</span><span class="n">c_fc</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">lkey1</span><span class="p">)</span>
         <span class="n">self</span><span class="p">.</span><span class="n">swiglu</span> <span class="o">=</span> <span class="nc">SwiGLU</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">skey</span><span class="p">)</span>
         <span class="n">self</span><span class="p">.</span><span class="n">c_proj</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">lkey2</span><span class="p">)</span>
         <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span>
    
     <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
         <span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">c_fc</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
         <span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">swiglu</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
         <span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">c_proj</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
         <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
         <span class="k">return</span> <span class="n">x</span>
</code></pre></div> </div> </li> </ol> <h3 id="causalselfattention-module">CausalSelfAttention Module</h3> <p>Moving forward, the process of converting modules should seem fairly straightforward since it mirrors the steps taken in the previous <em>MLP</em> module. We’ll however focus on pointing out the distinct alterations applied in the upcoming module definitions.</p> <h4 id="pytorch-version"><em>PyTorch</em> version:</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Code extracted from https://github.com/karpathy/nanoGPT/blob/master/model.py
</span>
<span class="k">class</span> <span class="nc">CausalSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span> <span class="o">%</span> <span class="n">config</span><span class="p">.</span><span class="n">n_head</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="c1"># key, query, value projections for all heads, but in a batch
</span>        <span class="n">self</span><span class="p">.</span><span class="n">c_attn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="c1"># output projection
</span>        <span class="n">self</span><span class="p">.</span><span class="n">c_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="c1"># regularization
</span>        <span class="n">self</span><span class="p">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">resid_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">n_head</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_embd</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">dropout</span>
        <span class="c1"># flash attention make GPU go brrrrr but support is only in PyTorch &gt;= 2.0
</span>        <span class="n">self</span><span class="p">.</span><span class="n">flash</span> <span class="o">=</span> <span class="nf">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">,</span> <span class="sh">'</span><span class="s">scaled_dot_product_attention</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">flash</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0</span><span class="sh">"</span><span class="p">)</span>
            <span class="c1"># causal mask to ensure that attention is only applied to the left in the input sequence
</span>            <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">"</span><span class="s">bias</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">))</span>
                                        <span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span> <span class="c1"># batch size, sequence length, embedding dimensionality (n_embd)
</span>
        <span class="c1"># calculate query, key, values for all heads in batch and move head forward to be the batch dim
</span>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span>  <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">c_attn</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">split</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># (B, nh, T, hs)
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># (B, nh, T, hs)
</span>        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">n_head</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># (B, nh, T, hs)
</span>
        <span class="c1"># causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">flash</span><span class="p">:</span>
            <span class="c1"># efficient attention using Flash Attention CUDA kernels
</span>            <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">training</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># manual implementation of attention
</span>            <span class="n">att</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">k</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
            <span class="n">att</span> <span class="o">=</span> <span class="n">att</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[:,:,:</span><span class="n">T</span><span class="p">,:</span><span class="n">T</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span>
            <span class="n">att</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">att</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn_dropout</span><span class="p">(</span><span class="n">att</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">att</span> <span class="o">@</span> <span class="n">v</span> <span class="c1"># (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)
</span>        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span> <span class="c1"># re-assemble all head outputs side by side
</span>
        <span class="c1"># output projection
</span>        <span class="n">y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">resid_dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">c_proj</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">y</span>
</code></pre></div></div> <h4 id="equinox-version"><em>Equinox</em> version:</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CausalSelfAttention</span><span class="p">(</span><span class="n">eqx</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">c_attn</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span>
    <span class="n">c_proj</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span>
    <span class="n">attn_dropout</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span>
    <span class="n">resid_dropout</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="nf">field</span><span class="p">(</span><span class="n">static</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">_config</span><span class="p">:</span> <span class="n">GPTConfig</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="nf">field</span><span class="p">(</span><span class="n">static</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span> <span class="o">%</span> <span class="n">config</span><span class="p">.</span><span class="n">n_head</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="c1"># PRNGKey
</span>        <span class="n">lkey1</span><span class="p">,</span> <span class="n">lkey2</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># key, query, value projections for all heads, but in a batch
</span>        <span class="n">self</span><span class="p">.</span><span class="n">c_attn</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">lkey1</span><span class="p">)</span>
        <span class="c1"># output projection
</span>        <span class="n">self</span><span class="p">.</span><span class="n">c_proj</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">lkey2</span><span class="p">)</span>
        <span class="c1"># regularization
</span>        <span class="n">self</span><span class="p">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">resid_dropout</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="c1"># causal mask to ensure that attention is only applied to the left in the input sequence
</span>        <span class="c1"># Has been made a buffer by using lax.stop_gradient whenever it is used.
</span>        <span class="c1"># Immutability calls for reshape, plus there is no view for jnp (or numpy) arrays.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">))).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span>
                                                                                       <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># sequence length, embedding dimensionality (n_embd)
</span>
        <span class="c1"># calculate query, key, values for all heads in batch and move head forward to be the batch dim
</span>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">c_attn</span><span class="p">)(</span><span class="n">x</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Immutability calls for reshape, plus there is no view for jnp (or numpy) arrays.
</span>        <span class="n">k</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">swapaxes</span><span class="p">(</span><span class="n">k</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">_config</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">_config</span><span class="p">.</span><span class="n">n_head</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># (nh, T, hs)
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">swapaxes</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">_config</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">_config</span><span class="p">.</span><span class="n">n_head</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># (nh, T, hs)
</span>        <span class="n">v</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">swapaxes</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">_config</span><span class="p">.</span><span class="n">n_head</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">_config</span><span class="p">.</span><span class="n">n_head</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># (nh, T, hs)
</span>
        <span class="c1"># manual implementation of attention
</span>        <span class="n">att</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">swapaxes</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">shape</span><span class="p">(</span><span class="n">k</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="c1"># Note: Added the stop_gradient just to be safe, I see no update rule acting on the bias inside the
</span>        <span class="c1"># forward pass.
</span>        <span class="n">att</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">lax</span><span class="p">.</span><span class="nf">stop_gradient</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">bias</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">T</span><span class="p">,</span> <span class="p">:</span><span class="n">T</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">),</span> <span class="n">att</span><span class="p">)</span>
        <span class="n">att</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">att</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn_dropout</span><span class="p">(</span><span class="n">att</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>  <span class="c1"># (nh, T, T) x (nh, T, hs) -&gt; (nh, T, hs)
</span>        <span class="c1"># Reshaping with Immutability creates a new copy
</span>        <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">swapaxes</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>  <span class="c1"># re-assemble all head outputs side by side
</span>
        <span class="c1"># output projection
</span>        <span class="n">y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">resid_dropout</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">c_proj</span><span class="p">)(</span><span class="n">y</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">y</span>
</code></pre></div></div> <ul> <li>We have rewritten the architecture of this attention module in the <code class="language-plaintext highlighter-rouge">__init__</code> method to look almost identical, with the exception of the last few lines.</li> <li>In this module, along with several subsequent ones, we register the <code class="language-plaintext highlighter-rouge">config</code> argument as a class field. This is a particular scenario where we are registering a field that does not constitute a layer in the <em>NN</em> architecture. In such a context, it becomes imperative to set it as an <em>Equinox</em> static field using <code class="language-plaintext highlighter-rouge">eqx.field(static=True)</code>.</li> <li>In the forward pass, you’ll notice we’ve changed <code class="language-plaintext highlighter-rouge">B, T, C = x.size()</code> to <code class="language-plaintext highlighter-rouge">B, T, C = jnp.size(x)</code>. This is an important difference that highlights the functional programming style of <em>JAX</em>. In PyTorch, tensors like <code class="language-plaintext highlighter-rouge">x</code> are objects with callable methods, so you would call the size method directly on <code class="language-plaintext highlighter-rouge">x</code>. But in <em>JAX</em>, arrays are passed as arguments to functions in <code class="language-plaintext highlighter-rouge">jax.numpy</code>. As we go through the code, keep an eye out for this functional pattern of passing arrays to <em>JAX</em> functions.</li> </ul> <div style=" background-color: #332e1b; border-radius: 5px; padding: 10px; margin: 0 0 0 30px; display: flex; align-items: flex-start; "> <img src="https://img.icons8.com/?size=100&amp;id=5tH5sHqq0t2q&amp;format=png&amp;color=000000" alt="Warning" style=" background-color: transparent; margin: 0 10px 0 0; height: 25px; border: none; align-self: flex-start; "/> <p style="margin: 0; line-height: 1.5; color: #b3bcc9;"> It’s important to note that while JAX is rooted in the functional programming paradigm and typically necessitates the passing of JAX arrays into functions as arguments, rather than invoking a method on the array object, it does incorporate certain functionalities as methods of the array for our convenience. A case in point is the <code>jax.numpy.transpose</code> function, which, in addition to its traditional use in functional programming, can also be invoked as a method on the JAX array. </p> </div> <ul> <li>So here’s the deal with <code class="language-plaintext highlighter-rouge">numpy</code> arrays (and by extension, <code class="language-plaintext highlighter-rouge">jax.numpy</code> arrays): they don’t come with a <code class="language-plaintext highlighter-rouge">view</code> method attached to them. To get our arrays into the shape we need for the transformations coming up next, we decided to use the handy <code class="language-plaintext highlighter-rouge">jnp.reshape</code> function.</li> <li>In our implementation, we skip the flash attention part and jump right into manually implementing the attention mechanism. You might notice some similarities between our approach and the original, aside from the fact that we’re using <em>JAX’s</em> functional API. <ul> <li>One key difference is that we use the <code class="language-plaintext highlighter-rouge">jnp.matmul</code> function to perform matrix multiplication, replacing the <code class="language-plaintext highlighter-rouge">@</code> operator.</li> <li>Another thing to watch out for is that <code class="language-plaintext highlighter-rouge">jnp.transpose</code> works a bit differently than <code class="language-plaintext highlighter-rouge">torch.transpose</code>. In <em>JAX</em>, <code class="language-plaintext highlighter-rouge">jnp.swapaxes</code> is the function you’ll want to use to achieve the same result as <em>PyTorch</em>.</li> </ul> </li> </ul> <h3 id="block-module">Block Module</h3> <p>Let’s take a closer look at the Block module, which is a key component of the transformer architecture. You’ll see that it uses almost all of the modules we defined earlier. One thing to note is that in the original <em>PyTorch</em> version, the author of <em>nanoGPT</em> passed in an argument for the <code class="language-plaintext highlighter-rouge">bias</code> parameter in the <em>LayerNorm</em> layer. If you were a <em>PyTorch</em> veteran (or simply referred the documentation), you might be gather that the built-in <em>LayerNorm</em> module doesn’t actually have this parameter! The author implemented their own custom LayerNorm from scratch to support this optional bias functionality. However, in our rewrite using the <em>Equinox</em> library, the built-in <em>LayerNorm</em> module conveniently includes a <code class="language-plaintext highlighter-rouge">bias</code> parameter by default, so we can use it directly without needing a custom implementation.</p> <h4 id="pytorch-version-1"><em>PyTorch</em> version:</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Code extracted from https://github.com/karpathy/nanoGPT/blob/master/model.py
</span>
<span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln_1</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn</span> <span class="o">=</span> <span class="nc">CausalSelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln_2</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">ln_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">ln_2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h4 id="equinox-version-1"><em>Equinox</em> version:</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">eqx</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">ln_1</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span>
    <span class="n">attn</span><span class="p">:</span> <span class="n">CausalSelfAttention</span>
    <span class="n">ln_2</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span>
    <span class="n">mlp</span><span class="p">:</span> <span class="n">MLP</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="n">ckey</span><span class="p">,</span> <span class="n">mkey</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">ln_1</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn</span> <span class="o">=</span> <span class="nc">CausalSelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">ckey</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln_2</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">mkey</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">ln_1</span><span class="p">)(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">ln_2</span><span class="p">)(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h3 id="gpt-module">GPT Module</h3> <p>We’ve now reached the top of our model structure. The original version had a lot of methods for this module, more than just the constructor (<code class="language-plaintext highlighter-rouge">__init__</code>) and <code class="language-plaintext highlighter-rouge">__call__</code> methods. But, we’ve cut down most of these methods to keep things simple and focus on the <em>JAX</em> and <em>Equinox</em> parts that we decided to implement in our code.</p> <h4 id="pytorch-version-2"><em>PyTorch</em> version:</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Code extracted from https://github.com/karpathy/nanoGPT/blob/master/model.py
</span>
<span class="k">class</span> <span class="nc">GPT</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">config</span><span class="p">.</span><span class="n">vocab_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
        <span class="k">assert</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
        <span class="n">self</span><span class="p">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="n">self</span><span class="p">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleDict</span><span class="p">(</span><span class="nf">dict</span><span class="p">(</span>
            <span class="n">wte</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">),</span>
            <span class="n">wpe</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">),</span>
            <span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nc">Block</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_layer</span><span class="p">)]),</span>
            <span class="n">ln_f</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">bias</span><span class="p">),</span>
        <span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="c1"># with weight tying when using torch.compile() some warnings get generated:
</span>        <span class="c1"># "UserWarning: functional_call was passed multiple values for tied weights.
</span>        <span class="c1"># This behavior is deprecated and will be an error in future versions"
</span>        <span class="c1"># not 100% sure what this is, so far seems to be harmless. TODO investigate
</span>        <span class="n">self</span><span class="p">.</span><span class="n">transformer</span><span class="p">.</span><span class="n">wte</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span><span class="p">.</span><span class="n">weight</span> <span class="c1"># https://paperswithcode.com/method/weight-tying
</span>
        <span class="c1"># init all weights
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_init_weights</span><span class="p">)</span>
        <span class="c1"># apply special scaled init to the residual projections, per GPT-2 paper
</span>        <span class="k">for</span> <span class="n">pn</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.():</span>
            <span class="k">if</span> <span class="n">pn</span><span class="p">.</span><span class="nf">endswith</span><span class="p">(</span><span class="sh">'</span><span class="s">c_proj.weight</span><span class="sh">'</span><span class="p">):</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="o">/</span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">n_layer</span><span class="p">))</span>

        <span class="c1"># report number of parameters
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">number of parameters: %.2fM</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">get_num_params</span><span class="p">()</span><span class="o">/</span><span class="mf">1e6</span><span class="p">,))</span>

    <span class="k">def</span> <span class="nf">get_num_params</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">non_embedding</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Return the number of parameters in the model.
        For non-embedding count (default), the position embeddings get subtracted.
        The token embeddings would too, except due to the parameter sharing these
        params are actually used as weights in the final layer, so we include them.
        </span><span class="sh">"""</span>
        <span class="n">n_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">non_embedding</span><span class="p">:</span>
            <span class="n">n_params</span> <span class="o">-=</span> <span class="n">self</span><span class="p">.</span><span class="n">transformer</span><span class="p">.</span><span class="n">wpe</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">n_params</span>

    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">module</span><span class="p">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">):</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="n">device</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">t</span> <span class="o">&lt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Cannot forward sequence of length </span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s">, block size is only </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="si">}</span><span class="sh">"</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="c1"># shape (t)
</span>
        <span class="c1"># forward the GPT model itself
</span>        <span class="n">tok_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">transformer</span><span class="p">.</span><span class="nf">wte</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1"># token embeddings of shape (b, t, n_embd)
</span>        <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">transformer</span><span class="p">.</span><span class="nf">wpe</span><span class="p">(</span><span class="n">pos</span><span class="p">)</span> <span class="c1"># position embeddings of shape (t, n_embd)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">transformer</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">tok_emb</span> <span class="o">+</span> <span class="n">pos_emb</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">transformer</span><span class="p">.</span><span class="n">h</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">transformer</span><span class="p">.</span><span class="nf">ln_f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># if we are given some desired targets also calculate the loss
</span>            <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">targets</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># inference-time mini-optimization: only forward the lm_head on the very last position
</span>            <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">:])</span> <span class="c1"># note: using list [-1] to preserve the time dim
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>

    <span class="nd">@torch.no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete
        the sequence max_new_tokens times, feeding the predictions back into the model each time.
        Most likely you</span><span class="sh">'</span><span class="s">ll want to make sure to be in model.eval() mode of operation for this.
        </span><span class="sh">"""</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
            <span class="c1"># if the sequence context is growing too long we must crop it at block_size
</span>            <span class="n">idx_cond</span> <span class="o">=</span> <span class="n">idx</span> <span class="k">if</span> <span class="n">idx</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span> <span class="k">else</span> <span class="n">idx</span><span class="p">[:,</span> <span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">:]</span>
            <span class="c1"># forward the model to get the logits for the index in the sequence
</span>            <span class="n">logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">idx_cond</span><span class="p">)</span>
            <span class="c1"># pluck the logits at the final step and scale by desired temperature
</span>            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">/</span> <span class="n">temperature</span>
            <span class="c1"># optionally crop the logits to only the top k options
</span>            <span class="k">if</span> <span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">v</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">topk</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="nf">min</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span> <span class="n">logits</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
                <span class="n">logits</span><span class="p">[</span><span class="n">logits</span> <span class="o">&lt;</span> <span class="n">v</span><span class="p">[:,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]]</span> <span class="o">=</span> <span class="o">-</span><span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">Inf</span><span class="sh">'</span><span class="p">)</span>
            <span class="c1"># apply softmax to convert logits to (normalized) probabilities
</span>            <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># sample from the distribution
</span>            <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># append sampled index to the running sequence and continue
</span>            <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_next</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

       <span class="k">return</span> <span class="n">idx</span>
</code></pre></div></div> <h4 id="equinox-version-2"><em>Equinox</em> version:</h4> <p>The original codebase defined the transformer layer as a dictionary of Modules (<code class="language-plaintext highlighter-rouge">ModuleDict</code> from <em>PyTorch</em>). However, since in <em>Equinox</em>, it is essential that we define the layers of the class as fields just before the constructor, it wasn’t possible to organize the code similar to the original structure. For this reason, as well as simplicity, we extracted the transformer layer into its own module, and we called it <code class="language-plaintext highlighter-rouge">TransformerLayer</code>.</p> <h5 id="transformerlayer-module"><code class="language-plaintext highlighter-rouge">TransformerLayer</code> Module</h5> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TransformerLayer</span><span class="p">(</span><span class="n">eqx</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">_config</span><span class="p">:</span> <span class="n">GPTConfig</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="nf">field</span><span class="p">(</span><span class="n">static</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">wte</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span>
    <span class="n">wpe</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span>
    <span class="n">drop</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span>
    <span class="n">h</span><span class="p">:</span> <span class="nb">list</span>
    <span class="n">ln_f</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="n">ekey</span><span class="p">,</span> <span class="n">pkey</span><span class="p">,</span> <span class="n">hkey</span><span class="p">,</span> <span class="n">fkey</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">config</span><span class="p">.</span><span class="n">vocab_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
        <span class="k">assert</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="n">self</span><span class="p">.</span><span class="n">wte</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">ekey</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">wpe</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">pkey</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">h</span> <span class="o">=</span> <span class="p">[</span><span class="nc">Block</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">hkey</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_layer</span><span class="p">)]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ln_f</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">t</span><span class="p">,</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="n">shape</span>
        <span class="k">assert</span> <span class="n">t</span> <span class="o">&lt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">_config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Cannot forward sequence of length </span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s">, block size is only </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">_config</span><span class="p">.</span><span class="n">block_size</span><span class="si">}</span><span class="sh">"</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span>

        <span class="n">tok_emb</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">wte</span><span class="p">)(</span><span class="n">idx</span><span class="p">)</span>  <span class="c1"># token embeddings of shape (t, n_embd)
</span>        <span class="n">pos_emb</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">wpe</span><span class="p">)(</span><span class="n">pos</span><span class="p">)</span>  <span class="c1"># position embeddings of shape (t, n_embd)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">tok_emb</span> <span class="o">+</span> <span class="n">pos_emb</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">h</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">ln_f</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p>We would like to draw the reader’s attention to the fact that in the first line of the forward pass, we are only capable of unpacking the token dimension length from the input. This is in contrast to the <em>PyTorch</em> implementation where the batch dimension is also obtained. The difference here arises from the fact that we won’t be processing a batch of inputs, but instead, a single input containing a sequence of tokens. <strong>DO NOT WORRY!!!</strong> This will become clear as we construct the training loop, where a vectorized map is applied on the batch dimension.</p> <p>With the transformer layer in a separate module, the <code class="language-plaintext highlighter-rouge">GPT</code> module is as simple as it can get. We show you the most minimal version of the <code class="language-plaintext highlighter-rouge">GPT</code> module below.</p> <h5 id="gpt-module-1"><code class="language-plaintext highlighter-rouge">GPT</code> Module</h5> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GPT</span><span class="p">(</span><span class="n">eqx</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">_config</span><span class="p">:</span> <span class="n">GPTConfig</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="nf">field</span><span class="p">(</span><span class="n">static</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">transformer</span><span class="p">:</span> <span class="n">TransformerLayer</span>
    <span class="n">lm_head</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="n">tkey</span><span class="p">,</span> <span class="n">lmhkey</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">config</span><span class="p">.</span><span class="n">vocab_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
        <span class="k">assert</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="n">self</span><span class="p">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="nc">TransformerLayer</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">tkey</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">lmhkey</span><span class="p">)</span>

        <span class="c1"># report number of parameters
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">number of parameters: %.2fM</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">get_num_params</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e6</span><span class="p">,))</span>

    <span class="k">def</span> <span class="nf">get_num_params</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">non_embedding</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Return the number of parameters in the model.
        For non-embedding count (default), the position embeddings get subtracted.
        The token embeddings would too, except due to the parameter sharing these
        params are actually used as weights in the final layer, so we include them.
        </span><span class="sh">"""</span>
        <span class="n">n_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree_util</span><span class="p">.</span><span class="nf">tree_leaves</span><span class="p">(</span><span class="n">eqx</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">eqx</span><span class="p">.</span><span class="n">is_array</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">non_embedding</span><span class="p">:</span>
            <span class="n">n_params</span> <span class="o">-=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">transformer</span><span class="p">.</span><span class="n">wpe</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">n_params</span>
    
    <span class="c1">## CODE STRIPPED FOR DEMONSTRATION
</span>    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">train_mode</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">train_mode</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">lm_head</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># inference-time mini-optimization: only forward the lm_head on the very last position
</span>            <span class="n">logits</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">lm_head</span><span class="p">)(</span><span class="n">x</span><span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">:])</span>  <span class="c1"># note: using list [-1] to preserve the time dim
</span>
        <span class="k">return</span> <span class="n">logits</span>
</code></pre></div></div> <p>In our <code class="language-plaintext highlighter-rouge">GPT</code> module’s forward pass, you may observe that we don’t design the method to take an optional <code class="language-plaintext highlighter-rouge">target</code> parameter, unlike the <em>PyTorch</em> implementation. In our version, we compute the loss within the training loop. More on that later. However, in this case, we instead accept a parameter to determine the mode in which the forward pass is invoked: training mode or inference mode. As a result, we can implement the appropriate logic during inference time, as seen in the original repo.</p> <p>Now, it’s only fair we show the reader how we implemented the rest of the logic in the original <code class="language-plaintext highlighter-rouge">GPT</code> module. We handle this task case-by-case, dividing sections for each method. For each of the methods, we follow a bottom-to-top approach here as well, by showing implementations of the all the dependencies and working our way up.</p> <p>We first define a helper package in our project to add some of the functional components that will help us implement certain logic in the <code class="language-plaintext highlighter-rouge">GPT</code> module faster, and more importantly: abstract the logic to bring it closer to <em>PyTorch</em>. We define two separate modules within the helper module as follows:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>.
└── helpers/
    ├── eqx.py
    └── init.py
</code></pre></div></div> <h5 id="initpy"><code class="language-plaintext highlighter-rouge">init.py</code></h5> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">normal_</span><span class="p">(</span><span class="n">array</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">mean</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">std</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">PRNGKey</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">new_array</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">array</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">std</span> <span class="o">+</span> <span class="n">mean</span>
    <span class="k">return</span> <span class="n">new_array</span>


<span class="k">def</span> <span class="nf">zeros_</span><span class="p">(</span><span class="n">array</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">new_array</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">numpy</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">array</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_array</span>
</code></pre></div></div> <p>While the second method stands to explain itself on its own, we explain the intent of the first function. It serves the purpose of initializing an input <em>JAX</em> array with a normal distribution with a given standard deviation and mean. This will come to be of use when initializing the <code class="language-plaintext highlighter-rouge">GPT</code> module.</p> <h5 id="eqxpy"><code class="language-plaintext highlighter-rouge">eqx.py</code></h5> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">named_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">path</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree_util</span><span class="p">.</span><span class="nf">tree_flatten_with_path</span><span class="p">(</span><span class="n">eqx</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">eqx</span><span class="p">.</span><span class="n">is_array</span><span class="p">))[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">pn</span> <span class="o">=</span> <span class="sh">''</span>

        <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">path</span><span class="p">)):</span>
            <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">path</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>  <span class="c1"># Check if path[index] is a string
</span>                <span class="n">pn</span> <span class="o">+=</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span> <span class="o">+</span> <span class="n">path</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">pn</span> <span class="o">+=</span> <span class="nf">str</span><span class="p">(</span><span class="n">path</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>

        <span class="n">out</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">pn</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">p</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">find_sub_tree</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">sub_tree_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">filter_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">path</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree_util</span><span class="p">.</span><span class="nf">tree_flatten_with_path</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">is_leaf</span><span class="o">=</span><span class="n">filter_fn</span><span class="p">)[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">pn</span> <span class="o">=</span> <span class="sh">''</span>
    
        <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">path</span><span class="p">)):</span>
            <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">path</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">jax</span><span class="p">.</span><span class="n">_src</span><span class="p">.</span><span class="n">tree_util</span><span class="p">.</span><span class="n">DictKey</span><span class="p">):</span>
                <span class="n">pn</span> <span class="o">+=</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span> <span class="o">+</span> <span class="n">path</span><span class="p">[</span><span class="n">index</span><span class="p">].</span><span class="n">key</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">pn</span> <span class="o">+=</span> <span class="nf">str</span><span class="p">(</span><span class="n">path</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
    
        <span class="k">if</span> <span class="n">filter_fn</span><span class="p">:</span>
            <span class="k">if</span> <span class="nf">filter_fn</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="ow">and</span> <span class="n">pn</span><span class="p">.</span><span class="nf">endswith</span><span class="p">(</span><span class="n">sub_tree_name</span><span class="p">):</span>
                <span class="n">out</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">pn</span><span class="p">.</span><span class="nf">endswith</span><span class="p">(</span><span class="n">sub_tree_name</span><span class="p">):</span>
            <span class="n">out</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> <p>In this module, the first function is written to replicate the function by the same name available as a method in the class <code class="language-plaintext highlighter-rouge">torch.Module</code> (read <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters">here</a>). It takes any <em>Equinox</em> module as an argument and returns a list of tuples, each containing a string representing the path to a parameter in the model and the parameter itself.</p> <p>The second function can be used to find a parameter whose full name ends with a given string. We shall see how these functions come in handy in just a few more sections.</p> <p>Circling back to the <code class="language-plaintext highlighter-rouge">GPT</code> module, focusing on the <code class="language-plaintext highlighter-rouge">_init_weights</code> method, you may notice that in the <em>PyTorch</em> version, this method serves as a custom initializer for the weights of the <em>Linear</em> and <em>Embedding</em> layers. If you look closely at the constructor, you’ll also see that right after this method is applied to the model, there’s another piece of custom initializer logic. This one is specifically for the residual projection weights (<code class="language-plaintext highlighter-rouge">c_proj.weight</code>). In our implementation, we’ve combined all these initializer logics into a single function as follows.</p> <h5 id="_init_weights-gpt-method"><code class="language-plaintext highlighter-rouge">_init_weights</code> <code class="language-plaintext highlighter-rouge">GPT</code> method</h5> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">eqx</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">GPTConfig</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">PRNGKey</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">init_layer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">is_layer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">mean</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">std</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="n">get_weights</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="p">[</span><span class="n">x</span><span class="p">.</span><span class="n">weight</span>
                                  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree_util</span><span class="p">.</span><span class="nf">tree_leaves</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">is_leaf</span><span class="o">=</span><span class="n">is_layer</span><span class="p">)</span>
                                  <span class="k">if</span> <span class="nf">is_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="nf">get_weights</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

        <span class="n">new_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">subkey</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">weight</span><span class="p">,</span> <span class="n">subkey</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)))]</span>

        <span class="k">return</span> <span class="n">eqx</span><span class="p">.</span><span class="nf">tree_at</span><span class="p">(</span><span class="n">get_weights</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">new_weights</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_linear</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
        <span class="n">is_linear</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">)</span>

        <span class="n">model</span> <span class="o">=</span> <span class="nf">init_layer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">is_linear</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

        <span class="n">get_biases</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="p">[</span><span class="n">x</span><span class="p">.</span><span class="n">bias</span>
                                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree_util</span><span class="p">.</span><span class="nf">tree_leaves</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">is_leaf</span><span class="o">=</span><span class="n">is_linear</span><span class="p">)</span>
                                <span class="k">if</span> <span class="nf">is_linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">and</span> <span class="n">x</span><span class="p">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">]</span>
        <span class="n">biases</span> <span class="o">=</span> <span class="nf">get_biases</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

        <span class="n">new_biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">init</span><span class="p">.</span><span class="nf">zeros_</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span> <span class="k">for</span> <span class="n">bias</span> <span class="ow">in</span> <span class="n">biases</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">eqx</span><span class="p">.</span><span class="nf">tree_at</span><span class="p">(</span><span class="n">get_biases</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">new_biases</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_embedding</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
        <span class="n">is_embedding</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">)</span>

        <span class="k">return</span> <span class="nf">init_layer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">is_embedding</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_c_proj_weights_with_normal</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
        <span class="n">get_c_proj_weights</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="n">eqx_helper</span><span class="p">.</span><span class="nf">find_sub_tree</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="sh">"</span><span class="s">c_proj.weight</span><span class="sh">"</span><span class="p">)</span>

        <span class="n">old_weights</span> <span class="o">=</span> <span class="nf">get_c_proj_weights</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">new_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">init</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">n_layer</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="n">subkey</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">weight</span><span class="p">,</span> <span class="n">subkey</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">old_weights</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">old_weights</span><span class="p">)))]</span>

        <span class="k">return</span> <span class="n">eqx</span><span class="p">.</span><span class="nf">tree_at</span><span class="p">(</span><span class="n">get_c_proj_weights</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">new_weights</span><span class="p">)</span>

    <span class="n">initialized_model</span> <span class="o">=</span> <span class="nf">init_linear</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">initialized_model</span> <span class="o">=</span> <span class="nf">init_embedding</span><span class="p">(</span><span class="n">initialized_model</span><span class="p">)</span>
    <span class="c1"># apply special scaled init to the residual projections, per GPT-2 paper
</span>    <span class="n">initialized_model</span> <span class="o">=</span> <span class="nf">init_c_proj_weights_with_normal</span><span class="p">(</span><span class="n">initialized_model</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">initialized_model</span>
</code></pre></div></div> <p>I know! You might be wondering how a few lines of <em>PyTorch</em> code turns into this. I assure you, this will sound simple once we breakdown the code into smaller blocks for explanation. We, however, remind the reader about the immutability of <em>JAX</em> arrays before proceeding. Hence any update to the model cannot be done therein, but instead returned as a new <em>PyTree</em>.</p> <p><strong><code class="language-plaintext highlighter-rouge">init_layer</code></strong> This function is written as an abstraction to allow initializing any layer that is filtered through the <code class="language-plaintext highlighter-rouge">is_layer</code> callable. It will initialize the layers of the input model matching the filter with values sampled from a normal distribution defined by the specified mean and standard deviation.</p> <div style=" background-color: #1c2b41; border-radius: 5px; padding: 10px; margin: 20px 0; display: flex; align-items: flex-start; "> <img src="https://cdn-uploads.huggingface.co/production/uploads/647eff9aaa8c04bbf9365219/S7GC6ed_inRwpFT2-S4sC.png" alt="Icon" style=" background-color: transparent; margin: 0 10px 0 0; height: 25px; border: none; align-self: flex-start; "/> <p style="margin: 0; line-height: 1.5; color: #b3bcc9;"> This code is nothing but a simple level of abstraction for the code found in the <i>Equinox</i> documentation for <i>Custom Parameter Initialization</i> (read <a href="https://docs.kidger.site/equinox/tricks/#custom-parameter-initialisation">here</a>). The reader is encouraged to refer to this documentation that we have also listed in our prerequisites section. </p> </div> <p><strong><code class="language-plaintext highlighter-rouge">init_linear</code></strong> Here, we simply call the <code class="language-plaintext highlighter-rouge">init_layer</code> with the filter to identify <em>Linear</em> layers in the model, and the returned model is then additionally initialized with zeros for the biases of the <em>Linear</em> layers.</p> <p><strong><code class="language-plaintext highlighter-rouge">init_embedding</code></strong> Very similar to the <code class="language-plaintext highlighter-rouge">init_linear</code> function.</p> <p><strong><code class="language-plaintext highlighter-rouge">init_c_proj_weights_with_normal</code></strong> Achieves the functionality as its name suggests. <code class="language-plaintext highlighter-rouge">c_proj.weights</code> are initialized with the custom normal distribution.</p> <p>We call these defined functions and return the new updated model. However, you may have noticed that even though we have defined this <code class="language-plaintext highlighter-rouge">_init_weights</code> method within the <code class="language-plaintext highlighter-rouge">GPT</code> module, it is not called in the constructor and hence will not do the necessary update to the model when an instance is created in the traditional sense. To achieve this, we create an additional static method that will be used to create a <code class="language-plaintext highlighter-rouge">GPT</code> instance with these updated weights.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">create_instance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="n">key1</span><span class="p">,</span> <span class="n">key2</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">inst</span> <span class="o">=</span> <span class="nc">GPT</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">key1</span><span class="p">)</span>
    <span class="n">new_inst</span> <span class="o">=</span> <span class="n">GPT</span><span class="p">.</span><span class="nf">_init_weights</span><span class="p">(</span><span class="n">inst</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">key2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">new_inst</span>
</code></pre></div></div> <div style=" background-color: #332e1b; border-radius: 5px; padding: 10px; margin: 20px 0; display: flex; align-items: flex-start; "> <img src="https://img.icons8.com/?size=100&amp;id=5tH5sHqq0t2q&amp;format=png&amp;color=000000" alt="Warning" style=" background-color: transparent; margin: 0 10px 0 0; height: 25px; border: none; align-self: flex-start; "/> <p style="margin: 0; line-height: 1.5; color: #b3bcc9;"> We avoid using the <code>_init_weight</code> to create the updated instance and simply replace the self object. Instead, we return a new instance that contains the updated weights. </p> </div> <p>To create a new instance of <code class="language-plaintext highlighter-rouge">GPT</code>, all we have to do is call <code class="language-plaintext highlighter-rouge">GPT.create_instance</code> instead of simply <code class="language-plaintext highlighter-rouge">GPT</code>. With this final method implemented, we come to an end of the <code class="language-plaintext highlighter-rouge">model.py</code> file. Now, moving onto the <code class="language-plaintext highlighter-rouge">train.py</code> file, where we show how this model is used in pretraining a language model from scratch.</p> <p>But first, let us try to understand how the vectorized map works in <em>JAX</em> in the following section. This concept is crucial for a reader to grasp how the training loop is built in the upcoming sections.</p> <hr/> <h3 id="understanding-the-vectorized-map-vmap-flow">Understanding the Vectorized Map (<code class="language-plaintext highlighter-rouge">vmap</code>) flow</h3> <p>In this section of this blog post, we intend to breakdown the flow of the input data to understand how the <code class="language-plaintext highlighter-rouge">vmap</code> works in each of the modules from top to bottom. We will use a loosely referenced mathematical notation to make things simpler.</p> <p>The input into the model will be a batch (ℬ) of tokens (𝒯) representing the text that will be used to pretrain the model.</p> <div style=" background-color: #1c2b41; border-radius: 5px; padding: 10px; margin: 20px 0; display: flex; align-items: flex-start; "> <img src="https://cdn-uploads.huggingface.co/production/uploads/647eff9aaa8c04bbf9365219/S7GC6ed_inRwpFT2-S4sC.png" alt="Icon" style=" background-color: transparent; margin: 0 10px 0 0; height: 25px; border: none; align-self: flex-start; "/> <p style="margin: 0; line-height: 1.5; color: #b3bcc9;"> This pretraining data can be a dataset of your choice, and you may follow the `prepare.py` scripts within the `data` folder to structure them to our training paradigm. </p> </div> <p>Hence the input would be a <code class="language-plaintext highlighter-rouge">jnp</code> array of shape,</p> <p>ℬ × 𝒯</p> <p>Since we will be passing this input to the model in the training script, we will call using the <code class="language-plaintext highlighter-rouge">vmap</code> transformation on the 0<sup>th</sup> dimension.</p> <div style="text-align: center;"> <code>jax.vmap(model, in_axes=(0, None))(x, True)</code> </div> <p>In the above code segment, recall that we have to define the batch dimension for every argument we pass into the vmap’d function. Hence, for the argument <code class="language-plaintext highlighter-rouge">x</code>, we indicate the 0<sup>th</sup> dimension and <code class="language-plaintext highlighter-rouge">None</code> for the second argument, <code class="language-plaintext highlighter-rouge">True</code>, to be the batch dimensions respectively.</p> <p>Now, looking at a very high level, the <code class="language-plaintext highlighter-rouge">GPT</code> module’s forward method only receives a token stream (𝒯), and the batch is executed parallelly as a series of individual functions.</p> <p>We then pass this 𝒯 through the transformer as <code class="language-plaintext highlighter-rouge">self.transformer(idx)</code>.</p> <p>The first two <em>Embedding</em> layers in the transformer will take in a scalar value and transform it into an embedding vector of the given size. However, we are trying to embed a stream of tokens, 𝒯, to obtain an embedded list of tokens corresponding to our initial input. Therefore, we need to batch <code class="language-plaintext highlighter-rouge">idx</code> across the 0th dimension so that the <em>Embedding</em> layer will be called with individual scalar values in 𝒯. The resulting array will then be of size 𝒯 × ℰ, <em>where ℰ is the number of embedding dimensions</em>.</p> <p>The same goes for the positional embedding as well. And the resulting array is passed through the <code class="language-plaintext highlighter-rouge">Block</code> module.</p> <p>In the <code class="language-plaintext highlighter-rouge">Block</code>’s forward pass, the layer normalization needs to be carried out on the embedding vector of each token. That is, the token dimension acts as a batch in this case. We apply <code class="language-plaintext highlighter-rouge">vmap</code> on the 0<sup>th</sup> dimension. The returned array is same as the input.</p> <p>The reader should now be equipped with sufficient experience to dissect the <code class="language-plaintext highlighter-rouge">vmap</code> process. We, therefore, leave it for the reader to explore the rest of the <code class="language-plaintext highlighter-rouge">vmap</code>s as an exercise.</p> <hr/> <h2 id="rewriting-trainpy">Rewriting <code class="language-plaintext highlighter-rouge">train.py</code></h2> <p>Now that we have completed building the model, we can move towards writing the training script. We will focus on the major code segments that lead up to the training process, allowing the rest to be self explanatory.</p> <h3 id="get_batch"><code class="language-plaintext highlighter-rouge">get_batch</code></h3> <p>This function will use the prepared bin files for the train/validation sets from executing the relevant dataset script found in the data folder. In our experiments, we execute the <code class="language-plaintext highlighter-rouge">prepare.py</code> file for the <em>tinystories</em> dataset.</p> <p>In the following function, we are randomly retrieving a batch of data of the specified size in a format suitable for pretraining the LLM.</p> <div style=" background-color: #1c2b41; border-radius: 5px; padding: 10px; margin: 20px 0; display: flex; align-items: flex-start; "> <img src="https://cdn-uploads.huggingface.co/production/uploads/647eff9aaa8c04bbf9365219/S7GC6ed_inRwpFT2-S4sC.png" alt="Icon" style=" background-color: transparent; margin: 0 10px 0 0; height: 25px; border: none; align-self: flex-start; "/> <p style="margin: 0; line-height: 1.5; color: #b3bcc9;"> Note that in this training exercise, the original repo intended to use a 600,000 batches to train the model, in contrast to the common convention of epochs. </p> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="n">split</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="c1"># We recreate np.memmap every batch to avoid a memory leak, as per
</span>    <span class="c1"># https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122
</span>    <span class="k">if</span> <span class="n">split</span> <span class="o">==</span> <span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">memmap</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="sh">'</span><span class="s">train.bin</span><span class="sh">'</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">memmap</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="sh">'</span><span class="s">validation.bin</span><span class="sh">'</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">ix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">block_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">block_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
</code></pre></div></div> <h3 id="convert_model_to_dtype"><code class="language-plaintext highlighter-rouge">convert_model_to_dtype</code></h3> <p>This function serves to convert our model, a <em>PyTree</em>, to a specified datatype. Note that we are using the globally defined datatype and simply overriding the global model as well. We call this function after initializing model in any of the three starting states: scratch, resume, or from gpt-2.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">convert_model_to_dtype</span><span class="p">():</span>
    <span class="k">global</span> <span class="n">model</span>
    <span class="k">def</span> <span class="nf">convert_pytree_to_dtype</span><span class="p">(</span><span class="n">pytree</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">_convert</span><span class="p">(</span><span class="n">leaf</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">eqx</span><span class="p">.</span><span class="nf">is_array</span><span class="p">(</span><span class="n">leaf</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">leaf</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">leaf</span>
    
        <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="n">tree_util</span><span class="p">.</span><span class="nf">tree_map</span><span class="p">(</span><span class="n">_convert</span><span class="p">,</span> <span class="n">pytree</span><span class="p">)</span>
    
    
    <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="sh">'</span><span class="s">bfloat16</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nf">convert_pytree_to_dtype</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">jnp</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dtype</span> <span class="o">==</span> <span class="sh">'</span><span class="s">float16</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nf">convert_pytree_to_dtype</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dtype</span> <span class="o">==</span> <span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nf">convert_pytree_to_dtype</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
</code></pre></div></div> <h3 id="lr_scheduler"><code class="language-plaintext highlighter-rouge">lr_scheduler</code></h3> <p>We define a simple cosine decay scheduler for the learning rate as follows. The <code class="language-plaintext highlighter-rouge">decay_steps</code> is defined so that when the training script is started with the intention of resuming the training process, the scheduler is aware of the remaining number of steps to decay the learning rate across.</p> <div style=" background-color: #332e1b; border-radius: 5px; padding: 10px; margin: 20px 0; display: flex; align-items: flex-start; "> <img src="https://img.icons8.com/?size=100&amp;id=5tH5sHqq0t2q&amp;format=png&amp;color=000000" alt="Warning" style=" background-color: transparent; margin: 0 10px 0 0; height: 25px; border: none; align-self: flex-start; "/> <p style="margin: 0; line-height: 1.5; color: #b3bcc9;"> This way of resuming a scheduler is not the most ideal or standard in deep learning practice. However, we proceed with such a rudimentary logic due to an unresolved error we faced while saving the optimizer state, ergo, the learning rate scheduler. We will be most thankful to a curious reader with a solution to saving and resuming the optimizer state of a an `Equinox` model. </p> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">optax</span><span class="p">.</span><span class="nf">warmup_cosine_decay_schedule</span><span class="p">(</span>
    <span class="n">init_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">peak_value</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_iters</span> <span class="k">if</span> <span class="n">init_from</span> <span class="o">==</span> <span class="sh">'</span><span class="s">scratch</span><span class="sh">'</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">decay_steps</span><span class="o">=</span><span class="n">lr_decay_iters</span> <span class="o">-</span> <span class="n">iter_num</span><span class="p">,</span>
    <span class="n">end_value</span><span class="o">=</span><span class="n">min_lr</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <h3 id="optimizer"><code class="language-plaintext highlighter-rouge">optimizer</code></h3> <p>We define a simple <em>AdamW</em> optimizer with <code class="language-plaintext highlighter-rouge">optax</code>. We have also used an <code class="language-plaintext highlighter-rouge">optax</code> wrapper, <code class="language-plaintext highlighter-rouge">inject_hyperparms</code>, so that we are able to access the current learning rate updated according the scheduler.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optax</span><span class="p">.</span><span class="nf">inject_hyperparams</span><span class="p">(</span><span class="n">optax</span><span class="p">.</span><span class="n">adamw</span><span class="p">)(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">lr_scheduler</span><span class="p">,</span> <span class="n">b1</span><span class="o">=</span><span class="n">beta1</span><span class="p">,</span> <span class="n">b2</span><span class="o">=</span><span class="n">beta2</span><span class="p">)</span>
</code></pre></div></div> <h3 id="compute_loss"><code class="language-plaintext highlighter-rouge">compute_loss</code></h3> <p>If you recall, we mentioned while defining the forward pass of the <code class="language-plaintext highlighter-rouge">GPT</code> module, that we will be calculating the loss within the training loop. This loss calculation is defined as a function as shown. This function is <em>JIT</em>’d with the <code class="language-plaintext highlighter-rouge">eqx.filter_jit</code> transformation as we are passing in an <em>Equinox</em> model into it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@eqx.filter_jit</span>
<span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">None</span><span class="p">))(</span><span class="n">x</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">optax</span><span class="p">.</span><span class="nf">softmax_cross_entropy_with_integer_labels</span><span class="p">(</span>
        <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="c1"># B, T, C
</span>        <span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="c1"># B, T
</span>    <span class="p">)</span>

    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</code></pre></div></div> <h3 id="make_step"><code class="language-plaintext highlighter-rouge">make_step</code></h3> <p>This is the top level function that is called within the training loop each iteration. This function executes a bunch of crucial steps for the model to train. We will attempt to break it down line-by-line.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@eqx.filter_jit</span>
<span class="k">def</span> <span class="nf">make_step</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">optimizer_state</span><span class="p">,</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">y</span>
<span class="p">):</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="nf">filter_value_and_grad</span><span class="p">(</span><span class="n">compute_loss</span><span class="p">)(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">updates</span><span class="p">,</span> <span class="n">optimizer_state</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">optimizer_state</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="nf">apply_updates</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer_state</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div> <h4 id="line-1">Line 1</h4> <p>The <code class="language-plaintext highlighter-rouge">compute_loss</code> function we wrote earlier is first transformed with the <code class="language-plaintext highlighter-rouge">filter_value_and_grad</code> function which will calculate the loss as well as the gradients for us. Here, we are conveniently executing the forward and backward pass in one single line.</p> <div style=" background-color: #1c2b41; border-radius: 5px; padding: 10px; margin: 20px 0; display: flex; align-items: flex-start; "> <img src="https://cdn-uploads.huggingface.co/production/uploads/647eff9aaa8c04bbf9365219/S7GC6ed_inRwpFT2-S4sC.png" alt="Icon" style=" background-color: transparent; margin: 0 10px 0 0; height: 25px; border: none; align-self: flex-start; "/> <p style="margin: 0; line-height: 1.5; color: #b3bcc9;"> The <code>eqx.filter_value_and_grad</code> function is <i>Equinox</i>’s implementation of the <code>jax.value_and_grad</code> transformation to account for the non <i>JAX</i> arrays present in the model. </p> </div> <h4 id="line-2">Line 2</h4> <p>With the calculated gradients, we compute the necessary updates for the model with the current optimizer state.</p> <h4 id="line-3">Line 3</h4> <p>The calculated updates are now applied to the model. This is the actual step that is taken towards reducing the model loss affected from the parameters.</p> <h4 id="line-4">Line 4</h4> <p>The updated model, optimizer state and the loss before making the step is returned to be accessed from the training loop.</p> <h3 id="estimate_loss"><code class="language-plaintext highlighter-rouge">estimate_loss</code></h3> <p>This function is written to calculate the training and evaluation loss at a fixed interval determined according to the training setup and is executed within the train loop.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">estimate_loss</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">eqx</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">val</span><span class="sh">'</span><span class="p">]:</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">eval_iters</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">eval_iters</span><span class="p">):</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">get_batch</span><span class="p">(</span><span class="n">split</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">stop_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">losses</span> <span class="o">=</span> <span class="n">losses</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="n">k</span><span class="p">].</span><span class="nf">set</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
        <span class="n">out</span><span class="p">[</span><span class="n">split</span><span class="p">]</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> <h3 id="the-train-loop">The Train Loop</h3> <p>We now show you the most minimal version of the training loop implemented in our code. After initializing the optimizer state, we make a step through every iteration. The loop is adapted to account for resuming stages as well. You may view the logging steps utilized in our project for an additional perspective.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer_state</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">eqx</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">eqx</span><span class="p">.</span><span class="n">is_array</span><span class="p">))</span>

<span class="k">for</span> <span class="n">local_iter_num</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iter_num</span><span class="p">,</span> <span class="n">max_iters</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">get_batch</span><span class="p">(</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># do a training step
</span>    <span class="n">model</span><span class="p">,</span> <span class="n">optimizer_state</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">make_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer_state</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <h3 id="saving-the-model">Saving the Model</h3> <p>We use the following logic to save the model parameters as well as the training configuration. We once again encourage the reader to refer our repo for the complete implementation of this logic.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">checkpoint_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">model_args</span><span class="sh">"</span><span class="p">:</span> <span class="n">gptconf</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">iter_num</span><span class="sh">"</span><span class="p">:</span> <span class="n">local_iter_num</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">val_loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">losses</span><span class="p">[</span><span class="sh">"</span><span class="s">val</span><span class="sh">"</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">learning_rate</span><span class="sh">"</span><span class="p">:</span> <span class="n">lr</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">config</span><span class="sh">"</span><span class="p">:</span> <span class="n">config</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">checkpoint_file</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="sh">'</span><span class="s">model.eqx</span><span class="sh">'</span><span class="p">)</span>
<span class="n">checkpoint_params_file</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">out_dir</span><span class="p">,</span> <span class="sh">'</span><span class="s">params.pkl</span><span class="sh">'</span><span class="p">)</span>

<span class="n">eqx</span><span class="p">.</span><span class="nf">tree_serialise_leaves</span><span class="p">(</span><span class="n">checkpoint_file</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">checkpoint_params_file</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">cloudpickle</span><span class="p">.</span><span class="nf">dump</span><span class="p">(</span><span class="n">checkpoint_params</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="conclusion">Conclusion</h2> <p>If you’ve reached this far through the sections, congratulations on your dedication to exploring <em>JAX</em> and <em>Equinox</em>! In this blog post, we’ve taken a unique approach to learning these powerful frameworks by rewriting the well-known <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a> repository step by step.</p> <p>Throughout this process, we’ve encountered and overcome several challenges unique to <em>JAX</em>’s immutable nature and <em>PyTree</em> definition. From reimagining the model architecture to adapting the training loop, each step helped us learn how to effectively leverage <em>JAX</em> and <em>Equinox</em> for complex deep learning tasks. We saw how to:</p> <ol> <li>Implement custom initializations.</li> <li>Handle model parameters as <em>PyTrees</em>.</li> <li>Use <em>Equinox</em>’s filtered transformations like <code class="language-plaintext highlighter-rouge">equinox.filter_jit</code> and <code class="language-plaintext highlighter-rouge">equinox.filter_grad</code> to work with non-array objects in our model.</li> </ol> <p>We’ve explored <em>JAX</em>’s transformations, particularly <code class="language-plaintext highlighter-rouge">vmap</code>, to create efficient, parallelized code for handling batched inputs across various layers of our model. <em>Equinox</em>’s ability to seamlessly integrate with <em>JAX</em> while providing a familiar <em>PyTorch</em>-like interface for building neural networks proved invaluable. Notably, <em>Equinox</em>’s filtered transformations were crucial in applying <em>JAX</em>’s powerful <em>JIT</em> compilation and automatic differentiation to our model, as we saw in the <code class="language-plaintext highlighter-rouge">compute_loss</code> and <code class="language-plaintext highlighter-rouge">make_step</code> functions.</p> <p>This rewrite not only serves as a learning exercise but also demonstrates the flexibility and power of <em>JAX</em> and <em>Equinox</em> in handling complex deep learning models. By working through this example, we hope you’ve gained a deeper understanding of these frameworks and feel more confident in applying them to your own projects.</p> <p>As we conclude, remember that this is just the beginning. The field of machine learning is constantly evolving, and frameworks like <em>JAX</em> and <em>Equinox</em> are only a pitstop in a never ending journey. We encourage you to continue exploring, experimenting, and pushing the boundaries of what’s possible with these tools and more.</p> <p>For those inspired to dive deeper, the entire codebase for this project is open-sourced and can be found <a href="https://github.com/surgeglobal/nanoJAXGPT">https://github.com/surgeglobal/nanoJAXGPT</a>. We hope this resource serves as a springboard for your own explorations in <em>JAX</em> and <em>Equinox</em>. May your journey in machine learning be filled with exciting discoveries and groundbreaking innovations!</p> <div style="display: flex; align-items: flex-start; background-color: #1e1e2e; padding: 16px; border-radius: 8px; font-family: Arial, sans-serif; color: white;"> <div style="margin-right: 12px;"> <img src="https://img.icons8.com/?size=100&amp;id=48250&amp;format=png&amp;color=000000" alt="repo-icon" style="width: 40px; height: 40px; margin: 0; background-color: transparent; border: none;"/> </div> <div> <h3 style="margin: 0; font-size: 15px;"><a href="https://github.com/surgeglobal/nanoJAXGPT" style="text-decoration: none; color: #7aa2f7;">surgeglobal/nanoJAXGPT</a></h3> <p style="margin: 4px 0; font-size: 14px; color: #a9b1d6;">Created by Surge Global • Updated on Jun 6, 2024</p> </div> </div> <h2 id="acknowledgements">Acknowledgements</h2> <ul> <li>We thank <a href="https://karpathy.ai/">Andrej Karpathy</a> for his elegent repository of <em>nanoGPT</em> which has helped us understand the <em>GPT</em> architecture and contribute with a <em>JAX/Equinox</em> version of their project.</li> <li>We are also grateful for <a href="https://github.com/anh-tong">Anh Tong</a> whose <em>Equinox</em> version of <em>nanoGPT</em> was a source of inspiration for our unique rewrite. We recommend referring to his version of nanoGPT as well here: <a href="https://github.com/anh-tong/nanoGPT-equinox">https://github.com/anh-tong/nanoGPT-equinox</a>.</li> <li>The <a href="https://jax.readthedocs.io/en/latest/index.html">JAX</a> team for an amazing framework.</li> <li>The <a href="https://docs.kidger.site/equinox/">Equinox</a> team for making JAX feel like PyTorch.</li> <li>The <a href="https://modal.com/">Modal</a> team for their effort in making serverless GPU usage accessible and affordable. Most importantly, for providing a free $30 credit for each workspace in your account.</li> <li>This blogpost is powered by free icons from <a href="https://icons8.com">Icons8</a>. <ul> <li><a target="_blank" href="https://icons8.com/icon/VQOfeAx5KWTK/info">Info</a> icon by <a target="_blank" href="https://icons8.com">Icons8</a></li> <li><a target="_blank" href="https://icons8.com/icon/hP6pCUyT8QGk/error">Warning</a> icon by <a target="_blank" href="https://icons8.com">Icons8</a></li> <li><a target="_blank" href="https://icons8.com/icon/48250/code">Code</a> icon by <a target="_blank" href="https://icons8.com">Icons8</a></li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="transformers"/><category term="llms"/><category term="python"/><category term="nanoGPT"/><summary type="html"><![CDATA[Since its introduction, JAX has seen a significant rise in popularity within the Machine Learning (ML) community. A simple web search would reveal the vast community support, a variety of derivative projects, and a multitude of Python libraries built around JAX. This leads to the inevitable question — What is JAX, and why should I care?]]></summary></entry></feed>